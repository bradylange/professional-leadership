page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("title") %>%
toTitleCase()
}
# -----------------------------------------------------------------------------
# Get Companies Method
# -----------------------------------------------------------------------------
# Retrieves the job companies on the specified page
get_companies <- function(page)
{
# Retrieve job companies from page
page %>%
html_nodes(xpath = "//span[@class='company']") %>% #"//a[@data-tn-element='companyName']")  %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Locations Method
# -----------------------------------------------------------------------------
# Retrieves the job locations on the specified page
get_locations <- function(page)
{
# Retrieve job locations from page
page %>%
html_nodes(xpath = "//*[starts-with(@class, 'location')]") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Card Links Method
# -----------------------------------------------------------------------------
# Retrieves the job card links on the specified page
get_card_links <- function(page)
{
# Retrieve job card links from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("href") %>%
lapply(., concatenate_indeed) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Card Method
# -----------------------------------------------------------------------------
# Retrieves the job card's HTML page from the specified link
get_card_html <- function(link)
{
# Link is an advertisement link
if (!is.na(str_extract(link, "pagead")))
{
# Open link and retrieve correct URL
link <- html_session(link) %>%
.$url
}
# Try retrieve card's HTML page and return NA on error
tryCatch(
{
# Retrieve card's HTML page
read_html(link) #%>%
# html_node(xpath = str_c("//*[@class='icl-Grid ",
#                         "jobsearch-ViewJobLayout-content ",
#                         "jobsearch-ViewJobLayout-mainContent ",
#                         "icl-u-lg-mt--md']"))
},
# On error return NA for bad HTML page
error = function(e)
{
NA
}
)
}
# -----------------------------------------------------------------------------
# Get Description Method
# -----------------------------------------------------------------------------
# Retrieves the job description from the specified job card
get_description <- function(card_html)
{
# Card page is not NA
if (!is.na(card_html))
{
# Retrieve card's job description and form sentences
description <- card_html %>%
html_node(xpath = "//*[@class='jobsearch-jobDescriptionText']") %>%
html_text(.) %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
# Job description text is in English, return the description text
if (!is.na(description) && textcat(description) == "english")
{
return(description)
}
# Job description text is not in English, return NA
else
{
return(NA)
}
}
# Card page is NA, return NA
else
{
return(NA)
}
}
# URL for website to be scraped - Query: Leadership terms, filter by
# Full-Time Job, display 50, and sort by date
first_page_url <- str_c("https://www.indeed.com/jobs?as_and&as_phr",
"&as_any=leadership%20leader%20lead%20supervise%20",
"supervisor%20manager%20manage%20administration%20",
"administrator%20authority%20control%20direction%20",
"influence%20initiative%20management%20power%20",
"capacity%20conduction%20conveyance%20directorship%20",
"domination%20foresight%20hegemony%20pilotage%20",
"preeminence%20primacy%20superiority%20supremacy%20",
"sway%20superintendency&as_not=Now%20hiring%20at%20",
"hiring%20event&as_ttl&as_cmp&jt=fulltime&st&as_src",
"&salary&radius=25&l&fromage=1&limit=50&sort=date",
"&psf=advsrch&vjk=7c43b6b0e1a8fa43")
# -----------------------------------------------------------------------------
# Scrape Jobs Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
scrape_jobs <- function(first_page_url)
{
# Start process time
process_time <<- proc.time()
# Retrieve the date and time of the jobs being scraped
scrape_time <- date()
# Format the date and time of the jobs being scraped
scrape_time <- str_replace_all(string = scrape_time,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# Company department/functions dictionary
department_dict <- tibble(department = c("general management", "marketing",
"operations", "finance", "sales",
"human resource", "purchase",
"information technology", "strategy",
"partner", "legal",
"reseach and development",
"customer service"))
# Retrieve the page URLs
page_urls <- get_page_urls(first_page_url)
# Retrieve the HTML pages of all of the pages
print("Retrieving all HTML pages...")
pages <- lapply(page_urls, read_html)
print("Retrieved all HTML pages...")
# Retrieve all of the job titles
print("Retrieving all job titles...")
titles <- lapply(pages, get_titles) %>%
unlist(.)
print("Retrieved all job titles...")
# Retrieve all of the companies
print("Retrieving all companies...")
companies <- lapply(pages, get_companies) %>%
unlist(.)
print("Retrieved all companies...")
# Retrieve all of the job locations
print("Retrieving all job locations...")
locations <- lapply(pages, get_locations) %>%
unlist(.)
print("Retrieved all job locations...")
# Retrieve all of the job card links
print("Retrieving all job card links...")
card_links <- lapply(pages, get_card_links) %>%
unlist(.)
print("Retrieved all job card links...")
# Retrieve all of the HTML pages of all of the job cards
print("Retrieving all job cards HTML...")
card_htmls <- lapply(card_links, get_card_html)
print("Retrieved all job cards HTML...")
# Retrieve all of the job descriptions
print("Retrieving all job descriptions...")
descriptions <- lapply(card_htmls, get_description) %>%
unlist(.)
print("Retrieved all job descriptions...")
# Combine all of the job vector data into a tibble with unique data
jobs <- tibble(title = titles, company = companies,
location = locations, description = descriptions) %>%
unique(.) %>%
na.omit(.)
# Load full data set of jobs
complete_jobs <- readRDS("./job_postings/jobs.RData")
# Pause the program to let data load
Sys.sleep(0.25)
# Combine the unique last day of jobs and the complete job data set
complete_jobs <- rbind(complete_jobs, jobs) %>%
unique(.) %>%
na.omit(.)
# Save the last day of jobs and complete jobs data sets
saveRDS(jobs, file = str_c("./job_postings/", scrape_time, ".RData"))
saveRDS(complete_jobs, file = "./job_postings/jobs.RData")
write.csv(complete_jobs, file = "./job_postings/jobs.csv", row.names = F)
# End the process time and calculate difference
process_time <- proc.time() - process_time
# Convert elapsed process time from seconds to minutes
process_time[3] <- round(process_time[3] / 60, 2)
# Load process time and average process time in minutes
load("./data/process_time.RData")
# Pause the program to let data load
Sys.sleep(0.05)
# Combine current process time and all process times
if (exists("all_proc_time") && nrow(all_proc_time) > 0)
{
all_proc_time <- all_proc_time %>%
add_row(id = as.numeric(tail(all_proc_time$id, n = 1)) + 1,
time = process_time[3])
}
else
{
all_proc_time <- tibble(id = as.character(1), time = process_time[3])
}
# Calculate average process time in minutes
time_avg <- mean(all_proc_time$time)
# Save all of the process times and process time average
save(all_proc_time, time_avg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", process_time[3], "\n",
"Average process time in minutes: ", time_avg))
# Close all connections
closeAllConnections()
return(jobs)
}
# Scrape Indeed.com for the last day of job postings
one_day_jobs <- scrape_jobs(first_page_url)
# Set-up workspace
graphics.off()
rm(list = ls())
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
library(textcat)
# =============================================================================
# Scrape Job Applications
# =============================================================================
# -----------------------------------------------------------------------------
# Concatenate Uniform Resource Locater Method
# -----------------------------------------------------------------------------
# Combines a given URL and page start number
concatenate_url <- function(page_start_num, url = first_page_url)
{
# Combine URL and page start number
str_c(url, page_start_num)
}
# -----------------------------------------------------------------------------
# Concatenate Indeed Method
# -----------------------------------------------------------------------------
# Combines Indeed's domain and URL path
concatenate_indeed <- function(url_path)
{
# Combine Indeed's URL and path
str_c("https://indeed.com", url_path)
}
# -----------------------------------------------------------------------------
# Get Uniform Resource Locaters Method
# -----------------------------------------------------------------------------
# Retrieves the navigation page URL's specified from the URL
get_page_urls <- function(url)
{
# Retrieve navigation page URLs
read_html(url) %>%
html_nodes(".pagination a") %>%
html_attr("href") %>%
str_extract(., "&start=\\d+") %>%
.[-length(.)] %>%
lapply(., concatenate_url) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Titles Method
# -----------------------------------------------------------------------------
# Retrieves the job titles on the specified page
get_titles <- function(page)
{
# Retrieve job titles from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("title") %>%
toTitleCase()
}
# -----------------------------------------------------------------------------
# Get Companies Method
# -----------------------------------------------------------------------------
# Retrieves the job companies on the specified page
get_companies <- function(page)
{
# Retrieve job companies from page
page %>%
html_nodes(xpath = "//span[@class='company']") %>% #"//a[@data-tn-element='companyName']")  %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Locations Method
# -----------------------------------------------------------------------------
# Retrieves the job locations on the specified page
get_locations <- function(page)
{
# Retrieve job locations from page
page %>%
html_nodes(xpath = "//*[starts-with(@class, 'location')]") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Card Links Method
# -----------------------------------------------------------------------------
# Retrieves the job card links on the specified page
get_card_links <- function(page)
{
# Retrieve job card links from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("href") %>%
lapply(., concatenate_indeed) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Card Method
# -----------------------------------------------------------------------------
# Retrieves the job card's HTML page from the specified link
get_card_html <- function(link)
{
# Link is an advertisement link
if (!is.na(str_extract(link, "pagead")))
{
# Open link and retrieve correct URL
link <- html_session(link) %>%
.$url
}
# Try retrieve card's HTML page and return NA on error
tryCatch(
{
# Retrieve card's HTML page
read_html(link) #%>%
# html_node(xpath = str_c("//*[@class='icl-Grid ",
#                         "jobsearch-ViewJobLayout-content ",
#                         "jobsearch-ViewJobLayout-mainContent ",
#                         "icl-u-lg-mt--md']"))
},
# On error return NA for bad HTML page
error = function(e)
{
NA
}
)
}
# -----------------------------------------------------------------------------
# Get Description Method
# -----------------------------------------------------------------------------
# Retrieves the job description from the specified job card
get_description <- function(card_html)
{
# Card page is not NA
if (!is.na(card_html))
{
# Retrieve card's job description and form sentences
description <- card_html %>%
html_node(xpath = "//*[@class='jobsearch-jobDescriptionText']") %>%
html_text(.) %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
# Job description text is in English, return the description text
if (!is.na(description) && textcat(description) == "english")
{
return(description)
}
# Job description text is not in English, return NA
else
{
return(NA)
}
}
# Card page is NA, return NA
else
{
return(NA)
}
}
# URL for website to be scraped - Query: Leadership terms, filter by
# Full-Time Job, display 50, and sort by date
first_page_url <- str_c("https://www.indeed.com/jobs?as_and&as_phr",
"&as_any=leadership%20leader%20lead%20supervise%20",
"supervisor%20manager%20manage%20administration%20",
"administrator%20authority%20control%20direction%20",
"influence%20initiative%20management%20power%20",
"capacity%20conduction%20conveyance%20directorship%20",
"domination%20foresight%20hegemony%20pilotage%20",
"preeminence%20primacy%20superiority%20supremacy%20",
"sway%20superintendency&as_not=Now%20hiring%20at%20",
"hiring%20event&as_ttl&as_cmp&jt=fulltime&st&as_src",
"&salary&radius=25&l&fromage=1&limit=50&sort=date",
"&psf=advsrch&vjk=7c43b6b0e1a8fa43")
# -----------------------------------------------------------------------------
# Scrape Jobs Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
scrape_jobs <- function(first_page_url)
{
# Start process time
process_time <<- proc.time()
# Retrieve the date and time of the jobs being scraped
scrape_time <- date()
# Format the date and time of the jobs being scraped
scrape_time <- str_replace_all(string = scrape_time,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# Company department/functions dictionary
department_dict <- tibble(department = c("general management", "marketing",
"operations", "finance", "sales",
"human resource", "purchase",
"information technology", "strategy",
"partner", "legal",
"reseach and development",
"customer service"))
# Retrieve the page URLs
page_urls <- get_page_urls(first_page_url)
# Retrieve the HTML pages of all of the pages
print("Retrieving all HTML pages...")
pages <- lapply(page_urls, read_html)
print("Retrieved all HTML pages...")
# Retrieve all of the job titles
print("Retrieving all job titles...")
titles <- lapply(pages, get_titles) %>%
unlist(.)
print("Retrieved all job titles...")
# Retrieve all of the companies
print("Retrieving all companies...")
companies <- lapply(pages, get_companies) %>%
unlist(.)
print("Retrieved all companies...")
# Retrieve all of the job locations
print("Retrieving all job locations...")
locations <- lapply(pages, get_locations) %>%
unlist(.)
print("Retrieved all job locations...")
# Retrieve all of the job card links
print("Retrieving all job card links...")
card_links <- lapply(pages, get_card_links) %>%
unlist(.)
print("Retrieved all job card links...")
# Retrieve all of the HTML pages of all of the job cards
print("Retrieving all job cards HTML...")
card_htmls <- lapply(card_links, get_card_html)
print("Retrieved all job cards HTML...")
# Retrieve all of the job descriptions
print("Retrieving all job descriptions...")
descriptions <- lapply(card_htmls, get_description) %>%
unlist(.)
print("Retrieved all job descriptions...")
# Combine all of the job vector data into a tibble with unique data
jobs <- tibble(title = titles, company = companies,
location = locations, description = descriptions) %>%
unique(.) %>%
na.omit(.)
# Load full data set of jobs
complete_jobs <- readRDS("./job_postings/jobs.RData")
# Pause the program to let data load
Sys.sleep(0.25)
# Combine the unique last day of jobs and the complete job data set
complete_jobs <- rbind(complete_jobs, jobs) %>%
unique(.) %>%
na.omit(.)
# Save the last day of jobs and complete jobs data sets
saveRDS(jobs, file = str_c("./job_postings/", scrape_time, ".RData"))
saveRDS(complete_jobs, file = "./job_postings/jobs.RData")
write.csv(complete_jobs, file = "./job_postings/jobs.csv", row.names = F)
# End the process time and calculate difference
process_time <- proc.time() - process_time
# Convert elapsed process time from seconds to minutes
process_time[3] <- round(process_time[3] / 60, 2)
# Load process time and average process time in minutes
load("./data/process_time.RData")
# Pause the program to let data load
Sys.sleep(0.05)
# Combine current process time and all process times
if (exists("all_proc_time") && nrow(all_proc_time) > 0)
{
all_proc_time <- all_proc_time %>%
add_row(id = as.numeric(tail(all_proc_time$id, n = 1)) + 1,
time = process_time[3])
}
else
{
all_proc_time <- tibble(id = as.character(1), time = process_time[3])
}
# Calculate average process time in minutes
time_avg <- mean(all_proc_time$time)
# Save all of the process times and process time average
save(all_proc_time, time_avg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", process_time[3], "\n",
"Average process time in minutes: ", time_avg))
# Close all connections
closeAllConnections()
return(jobs)
}
