}
else
{
allProcTime <- tibble(id = as.character(1), time = processTime[3])
}
# Calculate average process time in minutes
timeAvg <- mean(allProcTime$time)
# Save all of the process times and process time average
save(allProcTime, timeAvg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", processTime[3], "\n",
"Average process time in minutes: ", timeAvg))
return(jobs)
}
# Scrape Indeed.com for the last day of job postings
oneDayJobs <- scrapeJobs(firstPageURL)
?textcat
# Scrape Indeed.com for the last day of job postings
oneDayJobs <- scrapeJobs(firstPageURL)
load("./data/process_time.RData")
View(oneDayJobs)
View(allProcTime)
View(oneDayJobs)
?closeAllConnections
# Set-up workspace
graphics.off()
rm(list = ls())
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
library(textcat)
# =============================================================================
# Scrape Job Applications
# =============================================================================
# -----------------------------------------------------------------------------
# Concatenate Uniform Resource Locater Method
# -----------------------------------------------------------------------------
# Combines a given URL and page start number
concatenateURL <- function(pageStartNum, url = firstPageURL)
{
# Combine URL and page start number
str_c(url, pageStartNum)
}
# -----------------------------------------------------------------------------
# Concatenate Indeed Method
# -----------------------------------------------------------------------------
# Combines Indeed's domain and URL path
concatenateIndeed <- function(urlPath)
{
# Combine Indeed's URL and path
str_c("https://indeed.com", urlPath)
}
# -----------------------------------------------------------------------------
# Get Uniform Resource Locaters Method
# -----------------------------------------------------------------------------
# Retrieves the navigation page URL's specified from the URL
getPageURLs <- function(url)
{
# Retrieve navigation page URLs
read_html(url) %>%
html_nodes(".pagination a") %>%
html_attr("href") %>%
str_extract(., "&start=\\d+") %>%
.[-length(.)] %>%
lapply(., concatenateURL) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Titles Method
# -----------------------------------------------------------------------------
# Retrieves the job titles on the specified page
getTitles <- function(page)
{
# Retrieve job titles from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("title") %>%
toTitleCase()
}
# -----------------------------------------------------------------------------
# Get Companies Method
# -----------------------------------------------------------------------------
# Retrieves the job companies on the specified page
getCompanies <- function(page)
{
# Retrieve job companies from page
page %>%
html_nodes(xpath = "//*[@class='company']") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Locations Method
# -----------------------------------------------------------------------------
# Retrieves the job locations on the specified page
getLocations <- function(page)
{
# Retrieve job locations from page
page %>%
html_nodes(xpath = "//*[starts-with(@class, 'location')]") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Card Links Method
# -----------------------------------------------------------------------------
# Retrieves the job card links on the specified page
getCardLinks <- function(page)
{
# Retrieve job card links from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("href") %>%
lapply(., concatenateIndeed) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Card Method
# -----------------------------------------------------------------------------
# Retrieves the job card's HTML page from the specified link
getCardHTML <- function(link)
{
# Link is an advertisement link
if (!is.na(str_extract(link, "pagead")))
{
# Open link and retrieve correct URL
link <- html_session(link) %>%
.$url
}
# Try retrieve card's HTML page and return NA on error
tryCatch(
{
# Retrieve card's HTML page
read_html(link) #%>%
# html_node(xpath = str_c("//*[@class='icl-Grid ",
#                         "jobsearch-ViewJobLayout-content ",
#                         "jobsearch-ViewJobLayout-mainContent ",
#                         "icl-u-lg-mt--md']"))
},
# On error return NA for bad HTML page
error = function(e)
{
NA
}
)
}
# -----------------------------------------------------------------------------
# Get Description Method
# -----------------------------------------------------------------------------
# Retrieves the job description from the specified job card
getDescription <- function(cardHTML)
{
# Card page is not NA
if (!is.na(cardHTML))
{
# Retrieve card's job description and form sentences
description <- cardHTML %>%
html_node(xpath = "//*[@class='jobsearch-jobDescriptionText']") %>%
html_text(.) %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
# Job description text is in English, return the description text
if (textcat(description) == "english")
{
description
}
# Job description text is not in English, return NA
else
{
NA
}
}
# Card page is NA, return NA
else
{
NA
}
}
# URL for website to be scraped - Query: Leadership terms, filter by
# Full-Time Job, display 50, and sort by date
firstPageURL <- str_c("https://www.indeed.com/jobs?as_and&as_phr",
"&as_any=leadership%20leader%20lead%20supervise%20",
"supervisor%20manager%20manage%20administration%20",
"administrator%20authority%20control%20direction%20",
"influence%20initiative%20management%20power%20",
"capacity%20conduction%20conveyance%20directorship%20",
"domination%20foresight%20hegemony%20pilotage%20",
"preeminence%20primacy%20superiority%20supremacy%20",
"sway%20superintendency&as_not=Now%20hiring%20at%20",
"hiring%20event&as_ttl&as_cmp&jt=fulltime&st&as_src",
"&salary&radius=25&l&fromage=1&limit=50&sort=date",
"&psf=advsrch&vjk=7c43b6b0e1a8fa43")
# -----------------------------------------------------------------------------
# Scrape Jobs Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
scrapeJobs <- function(firstPageURL)
{
# Start process time
processTime <<- proc.time()
# Retrieve the date and time of the jobs being scraped
scrapeTime <- date()
# Format the date and time of the jobs being scraped
scrapeTime <- str_replace_all(string = scrapeTime,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# Company department/functions dictionary
departmentDict <- tibble(department = c("general management", "marketing",
"operations", "finance", "sales",
"human resource", "purchase",
"information technology", "strategy",
"partner", "legal",
"reseach and development",
"customer service"))
# Retrieve the page URLs
pageURLs <- getPageURLs(firstPageURL)
# Retrieve the HTML pages of all of the pages
print("Retrieving all HTML pages...")
pages <- lapply(pageURLs, read_html)
print("Retrieved all HTML pages...")
# Retrieve all of the job titles
print("Retrieving all job titles...")
titles <- lapply(pages, getTitles) %>%
unlist(.)
print("Retrieved all job titles...")
# Retrieve all of the companies
print("Retrieving all companies...")
companies <- lapply(pages, getCompanies) %>%
unlist(.)
print("Retrieved all companies...")
# Retrieve all of the job locations
print("Retrieving all job locations...")
locations <- lapply(pages, getLocations) %>%
unlist(.)
print("Retrieved all job locations...")
# Retrieve all of the job card links
print("Retrieving all job card links...")
cardLinks <- lapply(pages, getCardLinks) %>%
unlist(.)
print("Retrieved all job card links...")
# Retrieve all of the HTML pages of all of the job cards
print("Retrieving all job cards HTML...")
cardHTMLs <- lapply(cardLinks, getCardHTML)
print("Retrieved all job cards HTML...")
# Retrieve all of the job descriptions
print("Retrieving all job descriptions...")
descriptions <- lapply(cardHTMLs, getDescription) %>%
unlist(.)
print("Retrieved all job descriptions...")
# Combine all of the job vector data into a tibble with unique data
jobs <- tibble(title = titles, company = companies,
location = locations, description = descriptions) %>%
unique(.) %>%
na.omit(.)
# Load full data set of jobs
completeJobs <- readRDS("./job_postings/jobs.RData")
# Pause the program to let data load
Sys.sleep(0.25)
# Combine the unique last day of jobs and the complete job data set
completeJobs <- rbind(completeJobs, jobs) %>%
unique(.) %>%
na.omit(.)
# Save the last day of jobs and complete jobs data sets
saveRDS(jobs, file = str_c("./job_postings/", scrapeTime, ".RData"))
saveRDS(completeJobs, file = "./job_postings/jobs.RData")
write.csv(completeJobs, file = "./job_postings/jobs.csv", row.names = F)
# End the process time and calculate difference
processTime <- proc.time() - processTime
# Convert elapsed process time from seconds to minutes
processTime[3] <- round(processTime[3] / 60, 2)
# Load process time and average process time in minutes
load("./data/process_time.RData")
# Pause the program to let data load
Sys.sleep(0.05)
# Combine current process time and all process times
if (exists("allProcTime") && nrow(allProcTime) > 0)
{
allProcTime <- allProcTime %>%
add_row(id = as.numeric(tail(allProcTime$id, n = 1)) + 1,
time = processTime[3])
}
else
{
allProcTime <- tibble(id = as.character(1), time = processTime[3])
}
# Calculate average process time in minutes
timeAvg <- mean(allProcTime$time)
# Save all of the process times and process time average
save(allProcTime, timeAvg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", processTime[3], "\n",
"Average process time in minutes: ", timeAvg))
# Close all connections
closeAllConnections()
return(jobs)
}
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
library(textcat)
# Scrape Indeed.com for the last day of job postings
oneDayJobs <- scrapeJobs(firstPageURL)
# -----------------------------------------------------------------------------
# Scrape Jobs Limit Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
# based on a specific page start, page end, and jobs per page
scrapeJobsLimit <- function(url, pgStart, pgEnd, jobsPerPage)
{
scrapeTime <- date()
scrapeTime <- str_replace_all(string = scrapeTime,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# URL for website to be scraped - Query: All, filter by Full-Time Job,
# and sort by date
firstPageURL <- url
pgResults <- seq(pgStart, pgEnd, by = jobsPerPage)
ttlJobs <- data.frame()
for(i in seq_along(pgResults))
{
url <- str_c(firstPageURL, "&start=", pgResults[i])
# tryCatch(
# {
page <- read_html(url)
Sys.sleep(1)
# },
# error = function(e)
# {
#   page <- read_html(url)
#   Sys.sleep(3)
# })
# Sys.sleep pauses R for two seconds before it resumes
# Putting it there avoids error messages such as
# "Error in open.connection(con, "rb") : Timeout was reached"
#Sys.sleep(4)
# Title
jobTitle <- page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("title") %>%
toTitleCase()
# Company
jobCompany <- page %>%
html_nodes(xpath = "//*[@class='company']") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
# Location
jobLocation <- page %>%
html_nodes(xpath = "//*[@class='location accessible-contrast-color-location']") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
# Links
jobLinks <- page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
rvest::html_attr("href")
# Description
jobDescription <- c()
for(i in seq_along(jobLinks)) {
url <- str_c("https://indeed.com/", jobLinks[i])
# tryCatch(
# {
page <- read_html(url)
# },
# error = function(e)
# {
#   page <- read_html(url)
#   Sys.sleep(3)
# })
# Sys.sleep pauses R for two seconds before it resumes
# Putting it there avoids error messages such as
# "Error in open.connection(con, "rb") : Timeout was reached"
#Sys.sleep(2)
jobDescription[i] <- page %>%
html_node(xpath = str_c("//*[@class='jobsearch-JobComponent-",
"description icl-u-xs-mt--md']")) %>%
html_text() %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.])([a-z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_trim()
}
pgJobs <- tibble(jobTitle, jobCompany, jobLocation, jobDescription)
ttlJobs <- rbind(ttlJobs, pgJobs)
}
jobs <- readRDS("./job_postings/jobs.RData")
Sys.sleep(0.25)
jobs <- rbind(jobs, ttlJobs) %>%
unique(.)
saveRDS(ttlJobs, file = str_c("./job_postings/", scrapeTime, ".RData"))
saveRDS(jobs, file = "./job_postings/jobs.RData")
return(ttlJobs)
}
# -----------------------------------------------------------------------------
# Scrape Jobs Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
scrapeJobs <- function(firstPageURL)
{
# Start process time
processTime <<- proc.time()
# Retrieve the date and time of the jobs being scraped
scrapeTime <- date()
# Format the date and time of the jobs being scraped
scrapeTime <- str_replace_all(string = scrapeTime,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# Company department/functions dictionary
departmentDict <- tibble(department = c("general management", "marketing",
"operations", "finance", "sales",
"human resource", "purchase",
"information technology", "strategy",
"partner", "legal",
"reseach and development",
"customer service"))
# Retrieve the page URLs
pageURLs <- getPageURLs(firstPageURL)
# Retrieve the HTML pages of all of the pages
print("Retrieving all HTML pages...")
pages <- lapply(pageURLs, read_html)
print("Retrieved all HTML pages...")
# Retrieve all of the job titles
print("Retrieving all job titles...")
titles <- lapply(pages, getTitles) %>%
unlist(.)
print("Retrieved all job titles...")
# Retrieve all of the companies
print("Retrieving all companies...")
companies <- lapply(pages, getCompanies) %>%
unlist(.)
print("Retrieved all companies...")
# Retrieve all of the job locations
print("Retrieving all job locations...")
locations <- lapply(pages, getLocations) %>%
unlist(.)
print("Retrieved all job locations...")
# Retrieve all of the job card links
print("Retrieving all job card links...")
cardLinks <- lapply(pages, getCardLinks) %>%
unlist(.)
print("Retrieved all job card links...")
# Retrieve all of the HTML pages of all of the job cards
print("Retrieving all job cards HTML...")
cardHTMLs <- lapply(cardLinks, getCardHTML)
print("Retrieved all job cards HTML...")
# Retrieve all of the job descriptions
print("Retrieving all job descriptions...")
descriptions <- lapply(cardHTMLs, getDescription) %>%
unlist(.)
print("Retrieved all job descriptions...")
# Combine all of the job vector data into a tibble with unique data
jobs <- tibble(title = titles, company = companies,
location = locations, description = descriptions) %>%
unique(.) %>%
na.omit(.)
# Load full data set of jobs
completeJobs <- readRDS("./job_postings/jobs.RData")
# Pause the program to let data load
Sys.sleep(0.25)
# Combine the unique last day of jobs and the complete job data set
completeJobs <- rbind(completeJobs, jobs) %>%
unique(.) %>%
na.omit(.)
# Save the last day of jobs and complete jobs data sets
saveRDS(jobs, file = str_c("./job_postings/", scrapeTime, ".RData"))
saveRDS(completeJobs, file = "./job_postings/jobs.RData")
write.csv(completeJobs, file = "./job_postings/jobs.csv", row.names = F)
# End the process time and calculate difference
processTime <- proc.time() - processTime
# Convert elapsed process time from seconds to minutes
processTime[3] <- round(processTime[3] / 60, 2)
# Load process time and average process time in minutes
load("./data/process_time.RData")
# Pause the program to let data load
Sys.sleep(0.05)
# Combine current process time and all process times
if (exists("allProcTime") && nrow(allProcTime) > 0)
{
allProcTime <- allProcTime %>%
add_row(id = as.numeric(tail(allProcTime$id, n = 1)) + 1,
time = processTime[3])
}
else
{
allProcTime <- tibble(id = as.character(1), time = processTime[3])
}
# Calculate average process time in minutes
timeAvg <- mean(allProcTime$time)
# Save all of the process times and process time average
save(allProcTime, timeAvg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", processTime[3], "\n",
"Average process time in minutes: ", timeAvg))
# Close all connections
closeAllConnections()
return(jobs)
}
# Scrape Indeed.com for the last day of job postings
oneDayJobs <- scrapeJobs(firstPageURL)
# Scrape Indeed.com for the last day of job postings
oneDayJobs <- scrapeJobs(firstPageURL)
