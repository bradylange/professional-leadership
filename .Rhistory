str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
t
stri_enc_isascii(t)
stri_enc_isascii("©")
# -----------------------------------------------------------------------------
# Get Description Method
# -----------------------------------------------------------------------------
# Retrieves the job description from the specified job card
get_description <- function(card)
{
# Card page is not NA
if (!is.na(card))
{
# Retrieve card's job description and form sentences
description <- html_text(card) %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
# Job description text is in English, return the description text
if (stri_enc_isascii(description) == TRUE)
{
description
}
# Job description text is not in English, return NA
else
{
NA
}
}
# Card page is NA, return NA
else
{
NA
}
}
t <- read_html("https://www.indeed.com/viewjob?cmp=Hilton-Garden-Inn-Hotel&t=Housekeeping+Supervisor&jk=5847ba44f5472261&q=%28leadership+or+leader+or+lead+or+supervise+or+supervisor+or+manager+or+manage+or+administration+or+administrator+or+authority+or+control+or+direction+or+influence+or+initiative+or+management+or+power+or+capacity+or+conduction+or+conveyance+or+directorship&vjs=3") %>%
html_node(xpath = str_c("//*[@class='jobsearch-JobComponent-",
"description icl-u-xs-mt--md']"))
get_description(t)
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
# =============================================================================
# Scrape Job Applications
# =============================================================================
# -----------------------------------------------------------------------------
# Concatenate Uniform Resource Locater Method
# -----------------------------------------------------------------------------
# Combines a given URL and page start number
concatenate_url <- function(page_start_num, url = first_page_url)
{
# Combine URL and page start number
str_c(url, page_start_num)
}
# -----------------------------------------------------------------------------
# Concatenate Indeed Method
# -----------------------------------------------------------------------------
# Combines Indeed's domain and URL path
concatenate_indeed <- function(url_path)
{
# Combine Indeed's URL and path
str_c("https://indeed.com", url_path)
}
# -----------------------------------------------------------------------------
# Get Uniform Resource Locaters Method
# -----------------------------------------------------------------------------
# Retrieves the navigation page URL's specified from the URL
get_page_urls <- function(url)
{
# Retrieve navigation page URLs
read_html(url) %>%
html_nodes(".pagination a") %>%
html_attr("href") %>%
str_extract(., "&start=\\d+") %>%
.[-length(.)] %>%
lapply(., concatenate_url) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Titles Method
# -----------------------------------------------------------------------------
# Retrieves the job titles on the specified page
get_titles <- function(page)
{
# Retrieve job titles from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("title") %>%
toTitleCase()
}
# -----------------------------------------------------------------------------
# Get Companies Method
# -----------------------------------------------------------------------------
# Retrieves the job companies on the specified page
get_companies <- function(page)
{
# Retrieve job companies from page
page %>%
html_nodes(xpath = "//*[@class='company']") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Locations Method
# -----------------------------------------------------------------------------
# Retrieves the job locations on the specified page
get_locations <- function(page)
{
# Retrieve job locations from page
page %>%
html_nodes(xpath = "//*[@class='location']") %>%
html_text() %>%
toTitleCase() %>%
str_trim()
}
# -----------------------------------------------------------------------------
# Get Card Links Method
# -----------------------------------------------------------------------------
# Retrieves the job card links on the specified page
get_card_links <- function(page)
{
# Retrieve job card links from page
page %>%
html_nodes(xpath = "//a[@data-tn-element='jobTitle']") %>%
html_attr("href") %>%
lapply(., concatenate_indeed) %>%
unlist(.)
}
# -----------------------------------------------------------------------------
# Get Card Method
# -----------------------------------------------------------------------------
# Retrieves the job card's HTML page from the specified link
get_card <- function(link)
{
# Link is an advertisement link
if (!is.na(str_extract(link, "pagead")))
{
# Open link and retrieve correct URL
link <- html_session(link) %>%
.$url
}
# Try retrieve card's HTML page and return NA on error
tryCatch(
{
# Retrieve card's HTML page
read_html(link) %>%
html_node(xpath = str_c("//*[@class='jobsearch-JobComponent-",
"description icl-u-xs-mt--md']"))
},
# On error return NA for bad HTML page
error = function(e)
{
NA
}
)
}
# -----------------------------------------------------------------------------
# Get Description Method
# -----------------------------------------------------------------------------
# Retrieves the job description from the specified job card
get_description <- function(card)
{
# Card page is not NA
if (!is.na(card))
{
# Retrieve card's job description and form sentences
description <- html_text(card) %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
# Job description text is in English, return the description text
if (stri_enc_isascii(description) == TRUE)
{
description
}
# Job description text is not in English, return NA
else
{
NA
}
}
# Card page is NA, return NA
else
{
NA
}
}
# URL for website to be scraped - Query: Leadership terms, filter by
# Full-Time Job, display 50, and sort by date
first_page_url <- str_c("https://www.indeed.com/jobs?as_and&as_phr",
"&as_any=leadership%20leader%20lead%20supervise%20",
"supervisor%20manager%20manage%20administration%20",
"administrator%20authority%20control%20direction%20",
"influence%20initiative%20management%20power%20",
"capacity%20conduction%20conveyance%20directorship%20",
"domination%20foresight%20hegemony%20pilotage%20",
"preeminence%20primacy%20superiority%20supremacy%20",
"sway%20superintendency&as_not=Now%20hiring%20at%20",
"hiring%20event&as_ttl&as_cmp&jt=fulltime&st&as_src",
"&salary&radius=25&l&fromage=1&limit=50&sort=date",
"&psf=advsrch&vjk=7c43b6b0e1a8fa43")
# -----------------------------------------------------------------------------
# Scrape Jobs Method
# -----------------------------------------------------------------------------
# Scrapes job postings for their titles, companies, locations, and descriptions
scrape_jobs <- function(first_page_url)
{
# Start process time
process_time <<- proc.time()
# Retrieve the date and time of the jobs being scraped
scrape_time <- date()
# Format the date and time of the jobs being scraped
scrape_time <- str_replace_all(string = scrape_time,
pattern = " ",
replacement = "_") %>%
str_replace_all(pattern = ":", replacement = "-")
# Retrieve the page URLs
page_urls <- get_page_urls(first_page_url)
# Retrieve the HTML pages of all of the pages
pages <- lapply(page_urls, read_html)
# Retrieve all of the job titles
titles <- lapply(pages, get_titles) %>%
unlist(.)
# Retrieve all of the companies
companies <- lapply(pages, get_companies) %>%
unlist(.)
# Retrieve all of the job locations
locations <- lapply(pages, get_locations) %>%
unlist(.)
# Retrieve all of the job card links
card_links <- lapply(pages, get_card_links) %>%
unlist(.)
# Retrieve all of the HTML pages of all of the job cards
cards <- lapply(card_links, get_card)
# Retrieve all of the job descriptions
descriptions <- lapply(cards, get_description) %>%
unlist(.)
# Combine all of the job vector data into a tibble with unique data
jobs <- tibble(title = titles, company = companies,
location = locations, description = descriptions) %>%
unique(.) %>%
na.omit(.)
# Load full data set of jobs
complete_jobs <- readRDS("./job_postings/jobs.RData")
# Pause the program to let data load
Sys.sleep(0.25)
# Combine the unique last day of jobs and the complete job data set
complete_jobs <- rbind(complete_jobs, jobs) %>%
unique(.) %>%
na.omit(.)
# Save the last day of jobs and complete jobs data sets
saveRDS(jobs, file = str_c("./job_postings/", scrape_time, ".RData"))
saveRDS(complete_jobs, file = "./job_postings/jobs.RData")
write.csv(complete_jobs, file = "./job_postings/jobs.csv", row.names = F)
# End the process time and calculate difference
process_time <- proc.time() - process_time
# Convert elapsed process time from seconds to minutes
process_time[3] <- round(process_time[3] / 60, 2)
# Load process time and average process time in minutes
load("./data/process_time.RData")
# Pause the program to let data load
Sys.sleep(0.05)
# Combine current process time and all process times
all_proc_time <- rbind(all_proc_time, process_time)
# Calculate average process time in minutes
time_avg <- mean(all_proc_time$time)
# Save all of the process times and process time average
save(all_proc_time, time_avg, file = "./data/process_time.RData")
cat(str_c("Process time in minutes: ", process_time[3], "\n",
"Average process time in minutes: ", time_avg))
return(jobs)
}
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
# Retrieve file names
files <- list.files(path = "./job_postings", pattern = "\\d+.RData",
full.names = T, recursive = F)
for (file in files)
{
f <- readRDS(file)
f <- f %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
saveRDS(f, "./data/test")
}
warnings()
for (file in files)
{
f <- readRDS(file)
f$description <- f$description %>%
str_replace_all("([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
saveRDS(f, "./data/test")
}
files
for (file in files)
{
f <- readRDS(file)
f$description <-
str_replace_all(f$description, "([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
saveRDS(f, "./data/test")
}
for (file in files)
{
f <- readRDS(file)
f$description <-str_replace_all(f$description, "([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
saveRDS(f, "./data/test")
}
f <- readRDS(files[1])
f
f <- readRDS(files[1]) %>% as.tibble()
f
f$description <-str_replace_all(f$description, "([a-z]|[)])([A-Z])", "\\1. \\2") %>%
str_replace_all("([a-z][.]|:)([a-z]|[A-Z])", "\\1 \\2") %>%
str_replace_all("\r|\t|\n", " ") %>%
str_replace_all("©|®|™|°|–|—", "") %>%
str_replace_all("·", ".") %>%
str_replace_all("\\.\\.", ".") %>%
str_trim()
files[1]
for (file in files)
{
f <- readRDS(file)
tryCatch({
f <- tibble(title = job_title, company = job_company, location = job_location,
description = job_description)
},
error = function(e){
f <- tibble(title = title, company = company, location = location,
description = description)
})
saveRDS(f, file)
}
for (file in files)
{
f <- readRDS(file)
tryCatch({
f <- f %>%
tibble(title = job_title, company = job_company, location = job_location,
description = job_description)
},
error = function(e){
f <- f %>%
tibble(title = title, company = company, location = location,
description = description)
})
saveRDS(f, file)
}
f <- f %>%
tibble(title = job_title, company = job_company, location = job_location,
description = job_description)
f <- with(f,
tibble(title = job_title, company = job_company, location = job_location,
description = job_description))
f <- with(f,
tibble(title = job_title, company = job_company, location = job_location,
description = job_description)) %>%
as.character()
f <- with(f,
tibble(title = title, company = company, location = location,
description = description)) %>%
as.character()
f
f <- readRDS(files[1]) %>% as.tibble()
f <- with(f,
tibble(title = as.character(title), company = company, location = location,
description = description))
View(f)
typeof(f)
f <- readRDS(files[1]) %>% as_tibble()
f <- readRDS(files[1]) %>% as_tibble() %>% unlist
f <- readRDS(files[1]) %>% as_tibble()
class(f)
f$job_description %>% as.character()
f <- with(f,
tibble(title = as.character(title), company = company, location = location,
description = description))
f <- with(f,
tibble(title = as.character(job_title), company = job_company, location = job_location,
description = job_description))
for (file in files)
{
f <- readRDS(file)
tryCatch({
f <- with(f,
tibble(title = as.character(job_title), company = as.character(job_company),
location = as.character(job_location),
description = as.character(job_description))) %>%
as.character()
},
error = function(e){
f <- with(f,
tibble(title = as.character(title), company = as.character(company),
location = as.character(location),
description = as.character(description)))
})
saveRDS(f, file)
}
f <- readRDS(files[1]) %>% as_tibble()
f <- readRDS(files[1])
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
f <- readRDS(files[1])
# Retrieve file names
files <- list.files(path = "./job_postings", pattern = "\\d+.RData",
full.names = T, recursive = F)
f <- readRDS(files[1])
View(f)
View(f)
# Load libraries
library(tidyverse)
library(tidytext)
main_file <- list.files("./job_postings", pattern = "jobs.RData",
full.names = T, recursive = F)
# Load all jobs
jobs <- readRDS("./job_postings/jobs.RData")
# Convert job descriptions into tokens and count the frequency per job title
job_words <- jobs %>%
unnest_tokens(tokens, description) %>%
count(title, tokens, sort = T)
# Total the amount of tokens per job title
total_words <- job_words %>%
group_by(title) %>%
summarize(total = sum(n)) %>%
arrange(desc(total))
# Combine specific tokens per job title and total tokens per job title
job_words <- left_join(job_words, total_words)
# TF-IDF
tf_idf <- job_words %>%
bind_tf_idf(tokens, title, n)
# Leadership terms - reference (https://www.thesaurus.com/browse/leadership)
leadership_dict <- tibble(words = c("leadership", "leader", "lead",
"supervise", "supervisor",
"manager", "manage", "administration",
"administrator", "authority", "control",
"direction", "influence", "initiative",
"management", "power", "capacity",
"conduction", "conveyance", "directorship",
"domination", "foresight", "hegemony",
"pilotage", "preeminence", "primacy",
"superiority", "supremacy", "sway",
"superintendency"))
tf_idf
?tf_idf
# Load libraries
library(tidyverse)
library(tidytext)
?bind_tf_idf
# Retrieve leadership words and their TF-IDF
leader_words <- inner_join(tf_idf, leadership_dict,
by = c("tokens" = "words"))
leader_words
write.csv(leader_words, "leader_words.csv")
?write.csv
write.csv(leader_words, "leader_words.csv". row.names = F)
write.csv(leader_words, "leader_words.csv", row.names = F)
# Load all jobs
jobs <- readRDS("./job_postings/jobs.RData")
t <- unique(jobs$title)
t
job_titles <- unique(jobs$title)
write.csv(job_titles, "./data/job_titles.csv", row.names = F)
jobTitles <- tibble(title = unique(jobs$title))
# Load libraries
library(tidyverse)
library(stringi)
library(rvest)
library(tools)
jobTitles <- tibble(title = unique(jobs$title))
write.csv(jobTitles, "./data/job_titles.csv", row.names = F)
